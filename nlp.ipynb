{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b4929ea-b5d9-4f3d-b75a-b485a83cee83",
   "metadata": {},
   "source": [
    "## step_1 we need to configure the env and load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fe6f3a1c-4481-4574-a08c-c5368729dc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install huggingface-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8362219d-78a9-4448-a0be-196c222927f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b006238-faf6-4bae-a933-31b816a85306",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "print(\"Hugging Face Hub is working\")\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"stanfordnlp/imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9af68b-8773-4650-b899-142ec86aac21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0887fe6a-fc14-4d10-bf23-310df4083ac1",
   "metadata": {},
   "source": [
    "## step_2 check the structure and context of the dataset\n",
    "1)the dataset type is datasetdict that holds multiple datasets.In this case,including train test unsupervised.every dataset include 2 column.one is text\n",
    "the other is label .in label column 0 reprensent negative and 1 represent positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "24dbf81e-baa7-4c43-bab6-159fe3926a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a8ac6646-6209-486b-8064-2cbac65b213c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn't match the background, and painfully one-dimensional characters cannot be overcome with a 'sci-fi' setting. (I'm sure there are those of you out there who think Babylon 5 is good sci-fi TV. It's not. It's clich√©d and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It's really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it's rubbish as they have to always say \"Gene Roddenberry's Earth...\" otherwise people would not continue watching. Roddenberry's ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.\n"
     ]
    }
   ],
   "source": [
    "print(ds[\"test\"][\"text\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78126705-aeb2-411b-baba-558dd601ba50",
   "metadata": {},
   "source": [
    "## convert it into Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ac9a0695-3bcc-4dad-8dd4-60ed2c9a3663",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = ds['train'].to_pandas()\n",
    "test_df = ds['test'].to_pandas()\n",
    "unsupervised_df = ds['unsupervised'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "927654e8-ca88-4683-bae1-ae7d2d87c3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If only to avoid making this type of film in t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This film was probably inspired by Godard's Ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh, brother...after hearing about this ridicul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
       "1  \"I Am Curious: Yellow\" is a risible and preten...      0\n",
       "2  If only to avoid making this type of film in t...      0\n",
       "3  This film was probably inspired by Godard's Ma...      0\n",
       "4  Oh, brother...after hearing about this ridicul...      0"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6944bd13-5296-49de-837d-f14b7c12069c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\c\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "# Make sure to download the punkt tokenizer models if you haven't already\n",
    "nltk.download('punkt_tab')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1b97ba-6b5b-4952-b467-307029ba1c6c",
   "metadata": {},
   "source": [
    "## split it by sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9daa77fa-0f3d-4928-9c98-d76e9d668637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 sentence\n",
      "0       I rented I AM CURIOUS-YELLOW from my video sto...\n",
      "1       I also heard that at first it was seized by U....\n",
      "2       In particular she wants to focus her attention...\n",
      "3       In between asking politicians and ordinary den...\n",
      "4       Really, the sex and nudity scenes are few and ...\n",
      "...                                                   ...\n",
      "271052                           Time to end this review.\n",
      "271053  I have to go the dunny to shake hands with the...\n",
      "271054  The story centers around Barry McKenzie who mu...\n",
      "271055  Being about the grossest Aussie shearer ever t...\n",
      "271056  The songs of Barry McKenzie(Barry Crocker) are...\n",
      "\n",
      "[271057 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "def split_sentences(row):\n",
    "    sentences = sent_tokenize(row['text'])  # Tokenize the text into sentences\n",
    "    return pd.DataFrame({'sentence': sentences})\n",
    "\n",
    "# Apply the function to each row of the DataFrame and reset the index\n",
    "df_split = pd.concat([split_sentences(row) for _, row in train_df.iterrows()], ignore_index=True)\n",
    "\n",
    "# Display the new DataFrame\n",
    "print(df_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bdb21d-b332-4765-9c44-d4ddd7701099",
   "metadata": {},
   "source": [
    "## name column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dc9fdf-c3dc-4bea-b976-365d7780b582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_name(row):\n",
    "    if df_split('sentence')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f99f4dc-f44d-4bee-bc70-3316dd1f2133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/distilbert-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/distilbert-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84907de-379a-4078-b56a-85a8d0094a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_persons_from_ner(sentence):\n",
    "    ner_results = ner_pipeline(sentence)  # Apply NER to the sentence\n",
    "    # Extract only 'PERSON' entities\n",
    "    persons = [entity['word'] for entity in ner_results if entity['entity'] == 'B-PER' or entity['entity'] == 'I-PER']\n",
    "    return ' '.join(persons) if persons else None  # Join names if multiple, else return None\n",
    "df_split['name'] = df_split['sentence'].apply(lambda x: extract_persons_from_ner(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6e349bd4-fbe6-4125-a6b0-9655163ed1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I also heard that at first it was seized by U....</td>\n",
       "      <td>Lena</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In particular she wants to focus her attention...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In between asking politicians and ordinary den...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Really, the sex and nudity scenes are few and ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  name\n",
       "0  I rented I AM CURIOUS-YELLOW from my video sto...  None\n",
       "1  I also heard that at first it was seized by U....  Lena\n",
       "2  In particular she wants to focus her attention...  None\n",
       "3  In between asking politicians and ordinary den...  None\n",
       "4  Really, the sex and nudity scenes are few and ...  None"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_split.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6ac9e1b1-5f83-4215-a8d8-a8a13e09aea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence  name\n",
      "0  I rented I AM CURIOUS-YELLOW from my video sto...  None\n",
      "1  I also heard that at first it was seized by U....  Lena\n",
      "2  In particular she wants to focus her attention...  None\n"
     ]
    }
   ],
   "source": [
    "df_split = df_split.dropna(subset=['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c90605-26a6-42bf-9dc8-43eb27bcd876",
   "metadata": {},
   "source": [
    "## sentiment analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e657719b-34e3-4e09-bab9-7516d2303cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'#', '', text)  # Remove the # symbol\n",
    "    text = re.sub(r'https?:\\/\\/\\S+', '', text)  # Remove hyperlinks\n",
    "    text = text.replace(\"  \", \" \")  # Remove extra spaces\n",
    "    text = \" \".join([x for x in text.split(\" \") if not x.isdigit()])  # Remove numbers\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    return text\n",
    "df_split['sentence_normalized'] = df_split['sentence'].apply(lambda x: clean_text(x))\n",
    "\n",
    "df_split.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12faf1e-2e7e-433e-8079-1b1cf7a53b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dc5045-2323-49b5-9bb8-5c85ccb29de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd618656-e779-4c1d-9a67-4deeb12793b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0710dc-11cd-43de-9eb2-19726e8d1ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99fcc8b-4b99-439f-a02c-ff35971ab413",
   "metadata": {},
   "source": [
    "## group by name and rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c515817-14e0-4757-9651-766f0bb5ae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ce7c4e-779c-4e4a-9499-d9e122477a75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
